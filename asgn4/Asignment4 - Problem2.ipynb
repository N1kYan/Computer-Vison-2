{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import rgb_to_hsv\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from skimage.io import imshow\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as tf\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import VOC_LABEL2COLOR\n",
    "from utils import VOC_STATISTICS\n",
    "from utils import numpy2torch\n",
    "from utils import torch2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color2label(gt_in_colors):\n",
    "    # Transforming [H,W,C] label colors to [H,W] true labels\n",
    "    def get_label_from_color(rgb):\n",
    "        # Somehow imread() on a png file gives (x,y,4); We ignore the last entry which is always 255\n",
    "        label_color = tuple([rgb[0], rgb[1], rgb[2]])\n",
    "        # TODO: Make the lookup faster?\n",
    "        if label_color in VOC_LABEL2COLOR:\n",
    "            return VOC_LABEL2COLOR.index(label_color)\n",
    "        else:\n",
    "            return 0\n",
    "    gt_labels = np.apply_along_axis(get_label_from_color, 2, gt_in_colors)\n",
    "    return gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC Dataset class, derived from PyTorch Dataset\n",
    "class VOC2007Dataset(Dataset):\n",
    "    def __init__(self, root, train, num_examples):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reading num_examples lines from the split file for the image names\n",
    "        if train:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/train.txt\", \"r\")\n",
    "        else:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/val.txt\", \"r\")\n",
    "        splits = []\n",
    "        \n",
    "        # TODO: Read random line?\n",
    "        for _ in range(0, num_examples):\n",
    "            line = split_file.readline()\n",
    "            splits.append(line[:6])\n",
    "        split_file.close()\n",
    "                \n",
    "        # Reading pictures and gt for the entries in splits\n",
    "        # Saving a dictionary of {im, gt} for every entry in splits\n",
    "        self.data = []\n",
    "        c = 0\n",
    "        for name in splits:\n",
    "            print(\"{}/{}\".format(c+1, len(splits)), end=\"\\r\", flush=True)\n",
    "            self.data.append({\n",
    "                \"im\": numpy2torch(imread(root + \"/JPEGImages/{}.jpg\".format(name))),\n",
    "                # Colors in gt are converted to true labels\n",
    "                \"gt\": numpy2torch(color2label(imread(root + \"/SegmentationClass/{}.png\".format(name))))\n",
    "            })   \n",
    "            c += 1    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Returning random dictionary of {im, gt} from dataset\n",
    "        example_dict = np.random.choice(self.data)\n",
    "        assert (isinstance(example_dict, dict))\n",
    "        assert ('im' in example_dict.keys())\n",
    "        assert ('gt' in example_dict.keys())\n",
    "        return example_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return pytorch.utils DataLoader\n",
    "def create_loader(dataset, batch_size, shuffle, num_workers):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle, num_workers=num_workers)\n",
    "    assert (isinstance(loader, DataLoader))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Transforms RGB to HSV colorspace and adds label color.\n",
    "    \n",
    "    :param ndarray np_image: [H,W,C] integer numpy image\n",
    "    :param ndaraay np_label: [H,W] numpy array of true labels\n",
    "    :return : [H,W,C] numpy image \n",
    "\"\"\"\n",
    "def voc_label2color(np_image, np_label):\n",
    "    assert (isinstance(np_image, np.ndarray))\n",
    "    assert (isinstance(np_label, np.ndarray))\n",
    "\n",
    "    # Transform input into hsv space\n",
    "    # np_image gets casted to float at division\n",
    "    hsv_image = rgb_to_hsv(np_image/255.0)\n",
    "\n",
    "    # Iterate over all pixels\n",
    "    for x in range(0, np_label.shape[0]):\n",
    "        for y in range(0, np_label.shape[1]):\n",
    "            label_color = np.asarray(VOC_LABEL2COLOR[int(np_label[x, y])])\n",
    "            hsv_label_color = rgb_to_hsv(label_color/255.0)\n",
    "            hsv_image[x, y, 0] = hsv_label_color[0]\n",
    "            hsv_image[x, y, 1] = hsv_label_color[1]\n",
    "    \n",
    "    colored = hsv_to_rgb(hsv_image)\n",
    "\n",
    "    assert (np.equal(colored.shape, np_image.shape).all())\n",
    "    # assert (np_image.dtype == colored.dtype)\n",
    "    return colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset_examples(loader, grid_height, grid_width, title):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    c = 1\n",
    "    # Call the data loader until grid_height*grid_width samples are generated\n",
    "    for dic in loader:\n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "        else:\n",
    "            ax = fig.add_subplot(grid_height, grid_width, c)\n",
    "            colored_img = voc_label2color(torch2numpy(dic[\"im\"][0, :, :, :]), torch2numpy(dic[\"gt\"][0, :, :, :]))\n",
    "            ax.imshow(colored_img)\n",
    "            c += 1\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(input_tensor):\n",
    "    \n",
    "    # Normalize input by (x - mean(x)) / std(x) from VOC_STATISTICS\n",
    "    normalized = torch.zeros(size=input_tensor.shape)\n",
    "\n",
    "    transformation = transforms.Normalize(\n",
    "        mean=VOC_STATISTICS[\"mean\"],\n",
    "        std=VOC_STATISTICS[\"std\"]\n",
    "    )\n",
    "    \n",
    "    normalized = transformation(input_tensor[0, :, :, :].float()).unsqueeze(0)\n",
    "    \n",
    "    assert (type(input_tensor) == type(normalized))\n",
    "    assert (input_tensor.size() == normalized.size())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Run model forward pass and return label prediction of input image.\n",
    "    \n",
    "    :param tensor normalized: [1, C, H, W] float pytorch tensor of input image\n",
    "    :param model model: torchvision model used for inference\n",
    "    :return : [1, 1, H, W] prediction of true labels; [1, 21, H, W] tensor of activation values\n",
    "\"\"\"\n",
    "def run_forward_pass(normalized, model):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Activation function values: 21 x (height x width) tensors\n",
    "    # containing the predicted unnormalized probabilites for the 21 labels for each pixel of the normalized input\n",
    "    activations = model(normalized)['out']\n",
    "    \n",
    "    # ---------- Do a 'softmax' on the 21 labels dimension ----------\n",
    "    # (Assigning each pixel the most probable label)\n",
    "    prediction = torch.argmax(activations.squeeze(), dim=0).float()\n",
    "    \n",
    "    \"\"\"\n",
    "    prediction = torch.zeros(size=(1, 1, normalized.shape[2], normalized.shape[3]))\n",
    "\n",
    "    # Iterate over all pixels\n",
    "    for x in range(0, normalized.shape[2]):\n",
    "        for y in range(0,normalized.shape[3]):\n",
    "            # Get label with max. probability:\n",
    "            xy_label = torch.argmax(activations[0,:,x,y]).item()\n",
    "            # Assign label color to prediction\n",
    "            prediction[0,0,x,y] = xy_label            \n",
    "    \"\"\"\n",
    "    \n",
    "    assert (isinstance(prediction, torch.Tensor))\n",
    "    assert (isinstance(activations, torch.Tensor))\n",
    "    # Torch variables requiring grad need to be detached \n",
    "    return prediction.detach(), activations.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(prediction, gt):\n",
    "    sum_of_equal_elements = sum(sum((prediction.squeeze() == gt.squeeze())))\n",
    "    sum_of_elements = gt.squeeze().numel()\n",
    "    \n",
    "    return float(sum_of_equal_elements) / float(sum_of_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_examples(loader, model, grid_height, grid_width, title):\n",
    "        \n",
    "    c = 1\n",
    "    for dic in loader:\n",
    "       \n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "                \n",
    "        # Normalize current image\n",
    "        std_im = standardize_input(dic[\"im\"])\n",
    "        \n",
    "        # Ground truth\n",
    "        gt = dic[\"gt\"].float().squeeze()\n",
    "        \n",
    "        # Get label predictions for current image\n",
    "        # TODO: Predictions seem to be very bad ...\n",
    "        prediction, _ = run_forward_pass(std_im, model)\n",
    "        \n",
    "        # Get average precision\n",
    "        avp = average_precision(prediction, gt)\n",
    "        \n",
    "        # Plot predictions and ground truth\n",
    "        fig = plt.figure()\n",
    "        prediction = voc_label2color(torch2numpy(dic[\"im\"].squeeze()), torch2numpy(prediction.unsqueeze(0)))\n",
    "        true = voc_label2color(torch2numpy(dic[\"im\"].squeeze()), torch2numpy(gt.unsqueeze(0)))\n",
    "        ax_pred = fig.add_subplot(1, 2, 1)\n",
    "        ax_pred.imshow(prediction)\n",
    "        ax_pred.set_title(\"Prediction\")\n",
    "        ax_true = fig.add_subplot(1, 2, 2)\n",
    "        ax_true.imshow(true)\n",
    "        ax_true.set_title(\"Ground truth\")\n",
    "        fig.suptitle(\"AVP: \"+ str(round(avp,5)))\n",
    "        \n",
    "        c += 1\n",
    "        fig.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_example(loader, unique_foreground_label):\n",
    "    c = 1\n",
    "    example = {}\n",
    "    unique_labels = torch.tensor([0, unique_foreground_label])\n",
    "    for dic in loader:\n",
    "        labels = dic[\"gt\"].squeeze().unique()\n",
    "        if (labels.shape == unique_labels.shape):\n",
    "            if (labels == unique_labels).all():\n",
    "                print(\"Unique example found after {} iterations.\".format(c))\n",
    "                example = dic\n",
    "                break\n",
    "        c += 1\n",
    "            \n",
    "    assert (isinstance(example, dict))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_example(example_dict, model):\n",
    "    prediction, _ = run_forward_pass(standardize_input(example_dict[\"im\"]), model)\n",
    "    \n",
    "    gt = example_dict[\"gt\"].float().squeeze()\n",
    "    \n",
    "    avp = average_precision(prediction.squeeze(), gt)\n",
    "    \n",
    "    prediction = voc_label2color(torch2numpy(example_dict[\"im\"].squeeze()), torch2numpy(prediction.unsqueeze(0)))\n",
    "    true = voc_label2color(torch2numpy(example_dict[\"im\"].squeeze()), torch2numpy(gt.unsqueeze(0)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.suptitle(\"AVP: {}\".format(avp))\n",
    "    ax_pred = fig.add_subplot(1,2,1)\n",
    "    ax_pred.imshow(prediction)\n",
    "    ax_true = fig.add_subplot(1,2,2)\n",
    "    ax_true.imshow(true)\n",
    "    plt.show()    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attack(example_dict, model, src_label, target_label, learning_rate, iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set an environment variables 'VOC2007_HOME' pointing to your '../VOCdevkit/VOC2007' folder\n",
    "os.environ[\"VOC2007_HOME\"] = \"/home/yannik/Computer-Vison-2/asgn4/VOCdevkit/VOC2007\"\n",
    "root = os.environ[\"VOC2007_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets for training and validation\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = VOC2007Dataset(root, train=True, num_examples=128)\n",
    "print(\"\\n\")\n",
    "valid_dataset = VOC2007Dataset(root, train=False, num_examples=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders for training and validation\n",
    "# you can safely assume batch_size=1 in our tests..\n",
    "print(\"Creating data loaders...\")\n",
    "train_loader = create_loader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "valid_loader = create_loader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images for the training and validation set\n",
    "print(\"Dataset examples...\")\n",
    "show_dataset_examples(train_loader, grid_height=2, grid_width=3, title='training examples')\n",
    "show_dataset_examples(valid_loader, grid_height=2, grid_width=3, title='validation examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Deeplab network\n",
    "print(\"Loading deeplab network model...\")\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True, num_classes=21)\n",
    "\n",
    "# Apply deeplab. Switch to training loader if you want more variety.\n",
    "print(\"\\nInference with deeplab network model...\")\n",
    "show_inference_examples(valid_loader, model, grid_height=2, grid_width=3, title='inference examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack1: convert cat to dog\n",
    "print(\"Finding unique example...\")\n",
    "cat_example = find_unique_example(valid_loader, unique_foreground_label=8)\n",
    "show_unique_example(cat_example, model=model)\n",
    "#show_attack(cat_example, model, src_label=8, target_label=12, learning_rate=1.0, iterations=10)\n",
    "\n",
    "# feel free to try other examples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
