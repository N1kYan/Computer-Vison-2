{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from skimage.io import imshow\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as tf\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import VOC_LABEL2COLOR\n",
    "from utils import VOC_STATISTICS\n",
    "from utils import numpy2torch\n",
    "from utils import torch2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color2label(gt_in_colors):\n",
    "    # Transforming [H,W,C] label colors to [H,W,1] true labels\n",
    "    gt_labels = np.zeros(shape=(gt_in_colors.shape[0], gt_in_colors.shape[1], 1))\n",
    "    for x in range(0, gt_in_colors.shape[0]):\n",
    "        for y in range(0, gt_in_colors.shape[1]):\n",
    "            label_color = tuple(gt_in_colors[x,y,:])\n",
    "            if label_color in VOC_LABEL2COLOR:\n",
    "                gt_labels[x,y,0] = VOC_LABEL2COLOR.index(label_color)\n",
    "            else:\n",
    "                gt_labels[x,y,0] = 0\n",
    "    return gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC Dataset class, derived from PyTorch Dataset\n",
    "class VOC2007Dataset(Dataset):\n",
    "    def __init__(self, root, train, num_examples):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reading num_examples lines from the split file for the image names\n",
    "        if train:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/train.txt\", \"r\")\n",
    "        else:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/val.txt\", \"r\")\n",
    "        splits = []\n",
    "        \n",
    "        # TODO: Read random line?\n",
    "        for _ in range(0, num_examples):\n",
    "            line = split_file.readline()\n",
    "            splits.append(line[:6])\n",
    "        split_file.close()\n",
    "                \n",
    "        # Reading pictures and gt for the entries in splits\n",
    "        # Saving a dictionary of {im, gt} for every entry in splits\n",
    "        self.data = []\n",
    "        c = 0\n",
    "        for name in splits:\n",
    "            self.data.append({\n",
    "                \"im\" : numpy2torch(imread(root + \"/JPEGImages/{}.jpg\".format(name))),\n",
    "                # Colors in gt are converted to true labels\n",
    "                \"gt\" : numpy2torch(color2label(imread(root + \"/SegmentationClass/{}.png\".format(name))))\n",
    "            })     \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Returning random dictionary of {im, gt} from dataset\n",
    "        example_dict = np.random.choice(self.data)\n",
    "        assert (isinstance(example_dict, dict))\n",
    "        assert ('im' in example_dict.keys())\n",
    "        assert ('gt' in example_dict.keys())\n",
    "        return example_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return pytorch.utils DataLoader\n",
    "def create_loader(dataset, batch_size, shuffle, num_workers):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle, num_workers=num_workers)\n",
    "    assert (isinstance(loader, DataLoader))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_label2color(np_image, np_label):\n",
    "    assert (isinstance(np_image, np.ndarray))\n",
    "    assert (isinstance(np_label, np.ndarray))\n",
    "\n",
    "    colored = np.zeros(shape=np_image.shape, dtype=np_image.dtype)\n",
    "    \n",
    "    # Iterate over height\n",
    "    for x in range(0, np_image.shape[0]):\n",
    "        # Iterate over width\n",
    "        for y in range(0, np_image.shape[1]):\n",
    "            # Convert to greyscale PLUS the LABEL2COLOR values\n",
    "            colored[x, y, 0] = 0.30 * np_image[x, y, 0] + VOC_LABEL2COLOR[np_label[x, y, 0]][0]\n",
    "            colored[x, y, 1] = 0.59 * np_image[x, y, 1] + VOC_LABEL2COLOR[np_label[x, y, 0]][1]\n",
    "            colored[x, y, 2] = 0.11 * np_image[x, y, 2] + VOC_LABEL2COLOR[np_label[x, y, 0]][2]\n",
    "\n",
    "    assert (np.equal(colored.shape, np_image.shape).all())\n",
    "    assert (np_image.dtype == colored.dtype)\n",
    "    return colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset_examples(loader, grid_height, grid_width, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # TODO: Fix title and plots\n",
    "    plt.title(title)\n",
    "\n",
    "    c = 1\n",
    "    # Call the data loader until grid_height*grid_width samples are generated\n",
    "    for dic in loader:\n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "        else:\n",
    "            print(\"im: {} {}\".format(type(dic[\"im\"]), dic[\"im\"].shape))\n",
    "            print(\"gt: {} {}\".format(type(dic[\"gt\"]), dic[\"gt\"].shape))\n",
    "            plt.subplot(grid_height, grid_width, c)\n",
    "            colored_img = voc_label2color(torch2numpy(dic[\"im\"][0,:,:,:]), torch2numpy(dic[\"gt\"][0,:,:,:]))\n",
    "            plt.imshow(colored_img)\n",
    "            c += 1\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(input_tensor):\n",
    "    \n",
    "    # Normalize input by (x - mean(x)) / std(x) from VOC_STATISTICS\n",
    "    normalized = torch.zeros(size=input_tensor.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    normalized[0,0,:,:] = (input_tensor[0,0,:,:] - VOC_STATISTICS[\"mean\"][0]) / VOC_STATISTICS[\"std\"][0]\n",
    "    normalized[0,1,:,:] = (input_tensor[0,1,:,:] - VOC_STATISTICS[\"mean\"][1]) / VOC_STATISTICS[\"std\"][1]\n",
    "    normalized[0,2,:,:] = (input_tensor[0,2,:,:] - VOC_STATISTICS[\"mean\"][2]) / VOC_STATISTICS[\"std\"][2]\n",
    "    \"\"\"\n",
    "    transformation = transforms.Normalize(\n",
    "        mean = VOC_STATISTICS[\"mean\"],\n",
    "        std = VOC_STATISTICS[\"std\"]\n",
    "    )\n",
    "    \n",
    "    normalized[0,:,:,:] = transformation(input_tensor[0,:,:,:].float())\n",
    "    \n",
    "    assert (type(input_tensor) == type(normalized))\n",
    "    assert (input_tensor.size() == normalized.size())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forward_pass(normalized, model):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Activation function values: 21 x (height x width) tensors\n",
    "    # containing the predicted unnormalized probabilites for the 21 labels for each pixel of the normalized input\n",
    "    output = model(normalized)\n",
    "    acts = output['out']\n",
    "    \n",
    "    # ---------- Do a 'softmax' on the 21 labels dimension ----------\n",
    "    \n",
    "    prediction = torch.zeros(size=normalized.shape)\n",
    "\n",
    "    # Iterate over all pixels\n",
    "    for x in range(0, normalized.shape[2]):\n",
    "        for y in range(0,normalized.shape[3]):\n",
    "            # Get label with max. probability:\n",
    "            xy_label = torch.argmax(acts[0,:,x,y]).item()\n",
    "            # Assign label color to prediction\n",
    "            prediction[0,0,x,y] = VOC_LABEL2COLOR[xy_label][0]\n",
    "            prediction[0,1,x,y] = VOC_LABEL2COLOR[xy_label][1]\n",
    "            prediction[0,2,x,y] = VOC_LABEL2COLOR[xy_label][2]\n",
    "            \n",
    "\n",
    "    assert (isinstance(prediction, torch.Tensor))\n",
    "    assert (isinstance(acts, torch.Tensor))\n",
    "    \n",
    "    # Torch variables requiring grad need to be detached \n",
    "    # casting prediction to integer values\n",
    "    return prediction.detach(), acts.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(prediction, gt):\n",
    "    assert (prediction.shape == gt.shape)\n",
    "    assert (prediction.dtype == gt.dtype)\n",
    "    sum_of_equal_elements = float(sum(sum(sum(prediction[0,:,:,:] == gt[0,:,:,:]))))\n",
    "    sum_of_elements = gt[0,:,:,:].numel()\n",
    "    \n",
    "    return sum_of_equal_elements / sum_of_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_examples(loader, model, grid_height, grid_width, title):\n",
    "    plt.figure()\n",
    "    \n",
    "    c = 1\n",
    "    for dic in loader:\n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "        print(\"Iteration {} of show_inference_examples\".format(c))\n",
    "        plt.subplot(grid_height, grid_width, c)\n",
    "        \n",
    "        # Normalize current image\n",
    "        std_im = standardize_input(dic[\"im\"])\n",
    "        \n",
    "        # Ground truth\n",
    "        gt = dic[\"gt\"].float()\n",
    "        \n",
    "        # Get label predictions for current image\n",
    "        prediction, _ = run_forward_pass(std_im, model)\n",
    "        \n",
    "        # Get average precision\n",
    "        avp = average_precision(prediction, gt)\n",
    "        print(\"Average precision: \", avp)\n",
    "        \n",
    "        # Plot predictions\n",
    "        prediction = voc_label2color(torch2numpy(std_im[0,:,:,:]), torch2numpy(prediction[0,:,:,:]))\n",
    "        # TODO: also plot ground truth and arrange them nicely\n",
    "        plt.imshow(prediction)\n",
    "        \n",
    "        c += 1\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_example(loader, unique_foreground_label):\n",
    "    example = []\n",
    "\n",
    "    assert (isinstance(example, dict))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_example(example_dict, model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attack(example_dict, model, src_label, target_label, learning_rate, iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2():\n",
    "    # Please set an environment variables 'VOC2007_HOME' pointing to your '../VOCdevkit/VOC2007' folder\n",
    "    os.environ[\"VOC2007_HOME\"] = \"/home/yannik/Computer-Vison-2/asgn4/VOCdevkit/VOC2007\"\n",
    "    root = os.environ[\"VOC2007_HOME\"]\n",
    "\n",
    "    \n",
    "    # create datasets for training and validation\n",
    "    print(\"Creating datasets...\")\n",
    "    #train_dataset = VOC2007Dataset(root, train=True, num_examples=128)\n",
    "    train_dataset = VOC2007Dataset(root, train=True, num_examples=1)\n",
    "    #valid_dataset = VOC2007Dataset(root, train=False, num_examples=128)\n",
    "    valid_dataset = VOC2007Dataset(root, train=False, num_examples=1)\n",
    "    \n",
    "    # create data loaders for training and validation\n",
    "    # you can safely assume batch_size=1 in our tests..\n",
    "    print(\"\\nCreating data loaders...\")\n",
    "    train_loader = create_loader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "    valid_loader = create_loader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    # show some images for the training and validation set\n",
    "    print(\"\\nDataset examples...\")\n",
    "    show_dataset_examples(train_loader, grid_height=2, grid_width=3, title='training examples')\n",
    "    show_dataset_examples(valid_loader, grid_height=2, grid_width=3, title='validation examples')\n",
    "\n",
    "    # Load Deeplab network\n",
    "    print(\"\\nLoading deeplab network model...\")\n",
    "    #model = models.segmentation.deeplabv3_resnet101(pretrained=True, num_classes=21)\n",
    "\n",
    "    # Apply deeplab. Switch to training loader if you want more variety.\n",
    "    print(\"\\nInference with deeplab network model...\")\n",
    "    #show_inference_examples(valid_loader, model, grid_height=2, grid_width=3, title='inference examples')\n",
    "\n",
    "    \"\"\"\n",
    "    # attack1: convert cat to dog\n",
    "    cat_example = find_unique_example(valid_loader, unique_foreground_label=8)\n",
    "    show_unique_example(cat_example, model=model)\n",
    "    show_attack(cat_example, model, src_label=8, target_label=12, learning_rate=1.0, iterations=10)\n",
    "    \"\"\"\n",
    "    \n",
    "    # feel free to try other examples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "\n",
      "Creating data loaders...\n",
      "\n",
      "Dataset examples...\n",
      "im: <class 'torch.Tensor'> torch.Size([1, 3, 281, 500])\n",
      "gt: <class 'torch.Tensor'> torch.Size([1, 1, 281, 500])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-a0d56136345e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mproblem2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-3b6201431c54>\u001b[0m in \u001b[0;36mproblem2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# show some images for the training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDataset examples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mshow_dataset_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mshow_dataset_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-4601d68a30c2>\u001b[0m in \u001b[0;36mshow_dataset_examples\u001b[0;34m(loader, grid_height, grid_width, title)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gt: {} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mcolored_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc_label2color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch2numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"im\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch2numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-0112e0f51ff0>\u001b[0m in \u001b[0;36mvoc_label2color\u001b[0;34m(np_image, np_label)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Convert to greyscale PLUS the LABEL2COLOR values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mcolored\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.30\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVOC_LABEL2COLOR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mcolored\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.59\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVOC_LABEL2COLOR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mcolored\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.11\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVOC_LABEL2COLOR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    problem2()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
