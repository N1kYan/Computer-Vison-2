{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from skimage.io import imshow\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as tf\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import VOC_LABEL2COLOR\n",
    "from utils import VOC_STATISTICS\n",
    "from utils import numpy2torch\n",
    "from utils import torch2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC Dataset class, derived from PyTorch Dataset\n",
    "class VOC2007Dataset(Dataset):\n",
    "    def __init__(self, root, train, num_examples):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reading num_examples lines from the split file for the image names\n",
    "        if train:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/train.txt\", \"r\")\n",
    "        else:\n",
    "            split_file = open(root + \"/ImageSets/Segmentation/val.txt\", \"r\")\n",
    "        splits = []\n",
    "        \n",
    "        # TODO: Read random line?\n",
    "        for _ in range(0, num_examples):\n",
    "            line = split_file.readline()\n",
    "            splits.append(line[:6])\n",
    "        split_file.close()\n",
    "                \n",
    "        # Reading pictures and gt for the entries in splits\n",
    "        # Saving a dictionary of {im, gt} for every entry in splits\n",
    "        self.data = []\n",
    "        for name in splits:\n",
    "            self.data.append({\n",
    "                \"im\" : numpy2torch(imread(root + \"/JPEGImages/{}.jpg\".format(name))),\n",
    "                # TODO: Convert colors in gt to labels ?\n",
    "                \"gt\" : numpy2torch(imread(root + \"/SegmentationClass/{}.png\".format(name)))\n",
    "            })\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Returning random dictionary of {im, gt} from dataset\n",
    "        example_dict = np.random.choice(self.data)\n",
    "        assert (isinstance(example_dict, dict))\n",
    "        assert ('im' in example_dict.keys())\n",
    "        assert ('gt' in example_dict.keys())\n",
    "        return example_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return pytorch.utils DataLoader\n",
    "def create_loader(dataset, batch_size, shuffle, num_workers):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle, num_workers=num_workers)\n",
    "    assert (isinstance(loader, DataLoader))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_label2color(np_image, np_label):\n",
    "    assert (isinstance(np_image, np.ndarray))\n",
    "    assert (isinstance(np_label, np.ndarray))\n",
    "\n",
    "    colored = np.zeros(shape=np_image.shape, dtype=np_image.dtype)\n",
    "    \n",
    "    # Iterate over height\n",
    "    for a in range(0, np_image.shape[0]):\n",
    "        # Iterate over width\n",
    "        for b in range(0, np_image.shape[1]):\n",
    "            # Convert to greyscale PLUS the LABEL2COLOR values\n",
    "            \"\"\"\n",
    "            colored[0, a, b] = 0.3 * np_image[0, a, b] #+ VOC_LABEL2COLOR[np_label[a, b]][0]\n",
    "            colored[1, a, b] = 0.3 * np_image[1, a, b] #+ VOC_LABEL2COLOR[np_label[a, b]][1]\n",
    "            colored[2, a, b] = 0.3 * np_image[2, a, b] #+ VOC_LABEL2COLOR[np_label[a, b]][2]\n",
    "            \"\"\"\n",
    "            colored[a, b, 0] = 0.3 * np_image[a, b, 0] + np_label[a, b, 0]\n",
    "            colored[a, b, 1] = 0.59 * np_image[a, b, 1] + np_label[a, b, 1]\n",
    "            colored[a, b, 2] = 0.11 * np_image[a, b, 2] + np_label[a, b, 2]\n",
    "\n",
    "    assert (np.equal(colored.shape, np_image.shape).all())\n",
    "    assert (np_image.dtype == colored.dtype)\n",
    "    return colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset_examples(loader, grid_height, grid_width, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # TODO: Fix title and plots\n",
    "    plt.title(title)\n",
    "\n",
    "    c = 1\n",
    "    # Call the data loader until grid_height*grid_width samples are generated\n",
    "    for dic in loader:\n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "        else:\n",
    "            plt.subplot(grid_height, grid_width, c)\n",
    "            plt.plot(np.arange(0, 10), [x**2 for x in np.arange(0, 10)])\n",
    "            # TODO: Plot result from voc_label2color\n",
    "            colored_img = voc_label2color(torch2numpy(dic[\"im\"][0,:,:,:]), torch2numpy(dic[\"gt\"][0,:,:,:]))\n",
    "            plt.imshow(colored_img)\n",
    "            c += 1\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(input_tensor):\n",
    "    \n",
    "    # Normalize input by (x - mean(x)) / std(x) from VOC_STATISTICS\n",
    "    normalized = torch.zeros(size=input_tensor.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    normalized[0,0,:,:] = (input_tensor[0,0,:,:] - VOC_STATISTICS[\"mean\"][0]) / VOC_STATISTICS[\"std\"][0]\n",
    "    normalized[0,1,:,:] = (input_tensor[0,1,:,:] - VOC_STATISTICS[\"mean\"][1]) / VOC_STATISTICS[\"std\"][1]\n",
    "    normalized[0,2,:,:] = (input_tensor[0,2,:,:] - VOC_STATISTICS[\"mean\"][2]) / VOC_STATISTICS[\"std\"][2]\n",
    "    \"\"\"\n",
    "    transformation = transforms.Normalize(\n",
    "        mean = VOC_STATISTICS[\"mean\"],\n",
    "        std = VOC_STATISTICS[\"std\"]\n",
    "    )\n",
    "    \n",
    "    normalized[0,:,:,:] = transformation(input_tensor[0,:,:,:].float())\n",
    "    \n",
    "    assert (type(input_tensor) == type(normalized))\n",
    "    assert (input_tensor.size() == normalized.size())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forward_pass(normalized, model):\n",
    "    \n",
    "    prediction = []\n",
    "    acts = []\n",
    "\n",
    "    assert (isinstance(prediction, torch.Tensor))\n",
    "    assert (isinstance(acts, torch.Tensor))\n",
    "    return prediction, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(prediction, gt):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_examples(loader, model, grid_height, grid_width, title):\n",
    "    model.eval()\n",
    "    c = 1\n",
    "    for dic in loader:\n",
    "        if c > grid_height*grid_width:\n",
    "            break\n",
    "        # Normalize current image\n",
    "        std_im = standardize_input(dic[\"im\"])\n",
    "        print(\"std_im: {} {}\".format(std_im.dtype, std_im.shape))\n",
    "        print(type(model(std_im)))\n",
    "        c += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_example(loader, unique_foreground_label):\n",
    "    example = []\n",
    "\n",
    "    assert (isinstance(example, dict))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_example(example_dict, model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attack(example_dict, model, src_label, target_label, learning_rate, iterations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2():\n",
    "    # Please set an environment variables 'VOC2007_HOME' pointing to your '../VOCdevkit/VOC2007' folder\n",
    "    os.environ[\"VOC2007_HOME\"] = \"/home/yannik/Computer-Vison-2/asgn4/VOCdevkit/VOC2007\"\n",
    "    root = os.environ[\"VOC2007_HOME\"]\n",
    "\n",
    "    \n",
    "    # create datasets for training and validation\n",
    "    #train_dataset = VOC2007Dataset(root, train=True, num_examples=128)\n",
    "    train_dataset = VOC2007Dataset(root, train=True, num_examples=32)\n",
    "    #valid_dataset = VOC2007Dataset(root, train=False, num_examples=128)\n",
    "    valid_dataset = VOC2007Dataset(root, train=False, num_examples=32)\n",
    "    \n",
    "    # create data loaders for training and validation\n",
    "    # you can safely assume batch_size=1 in our tests..\n",
    "    train_loader = create_loader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "    valid_loader = create_loader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    \"\"\"\n",
    "    # show some images for the training and validation set\n",
    "    show_dataset_examples(train_loader, grid_height=2, grid_width=3, title='training examples')\n",
    "    show_dataset_examples(valid_loader, grid_height=2, grid_width=3, title='validation examples')\n",
    "\n",
    "    \"\"\"\n",
    "    # Load Deeplab network\n",
    "    model = models.segmentation.deeplabv3_resnet101(pretrained=True, num_classes=21)\n",
    "\n",
    "    # Apply deeplab. Switch to training loader if you want more variety.\n",
    "    show_inference_examples(valid_loader, model, grid_height=2, grid_width=3, title='inference examples')\n",
    "\n",
    "    \"\"\"\n",
    "    # attack1: convert cat to dog\n",
    "    cat_example = find_unique_example(valid_loader, unique_foreground_label=8)\n",
    "    show_unique_example(cat_example, model=model)\n",
    "    show_attack(cat_example, model, src_label=8, target_label=12, learning_rate=1.0, iterations=10)\n",
    "    \"\"\"\n",
    "    \n",
    "    # feel free to try other examples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_im: torch.float32 torch.Size([1, 3, 375, 500])\n",
      "<class 'collections.OrderedDict'>\n",
      "std_im: torch.float32 torch.Size([1, 3, 500, 375])\n",
      "<class 'collections.OrderedDict'>\n",
      "std_im: torch.float32 torch.Size([1, 3, 375, 500])\n",
      "<class 'collections.OrderedDict'>\n",
      "std_im: torch.float32 torch.Size([1, 3, 500, 375])\n",
      "<class 'collections.OrderedDict'>\n",
      "std_im: torch.float32 torch.Size([1, 3, 375, 500])\n",
      "<class 'collections.OrderedDict'>\n",
      "std_im: torch.float32 torch.Size([1, 3, 375, 500])\n",
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    problem2()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
