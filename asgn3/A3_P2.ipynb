{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as tf\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import flow2rgb\n",
    "from utils import rgb2gray\n",
    "from utils import read_flo\n",
    "from utils import read_image\n",
    "from torch.autograd import variable\n",
    "\n",
    "from scipy.integrate import dblquad\n",
    "from numpy.linalg import norm\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy2torch(array):\n",
    "    \"\"\" Converts 3D numpy HWC ndarray to 3D PyTorch CHW tensor.\"\"\"\n",
    "    assert (array.ndim == 3)\n",
    "\n",
    "    result = array\n",
    "    result = np.moveaxis(result,[1,2,0],[2,0,1])\n",
    "    result = torch.from_numpy(result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch2numpy(tensor):\n",
    "    \"\"\" Convert 3D PyTorch CHW tensor to 3D numpy HWC ndarray.\"\"\"\n",
    "    assert (tensor.dim() == 3)\n",
    "\n",
    "    result = tensor.permute(1,2,0).numpy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(im1_filename, im2_filename, flo_filename):\n",
    "    \"\"\" Loads images and flow ground truth. Returns 4D tensors.\"\"\"\n",
    "    i0_path = os.path.dirname(os.path.abspath(\"A3_P2\"))+\"/\"+im1_filename\n",
    "    i1_path = os.path.dirname(os.path.abspath(\"A3_P2\"))+\"/\"+im2_filename\n",
    "    flo_path = os.path.dirname(os.path.abspath(\"A3_P2\"))+\"/\"+flo_filename\n",
    "    i0 = read_image(i0_path)\n",
    "    i0 = rgb2gray(i0)\n",
    "    i1 = read_image(i1_path)\n",
    "    i1 = rgb2gray(i1)    \n",
    "    tensor1 = numpy2torch(i0).view(1,1,i0.shape[0],i0.shape[1])\n",
    "    tensor2 = numpy2torch(i1).view(1,1,i1.shape[0],i1.shape[1])\n",
    "    flow_gt = numpy2torch(read_flo(flo_path)).view(1,2,i1.shape[0],i1.shape[1])\n",
    "\n",
    "    return tensor1, tensor2, flow_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_flow(flow, flow_gt):\n",
    "    \"\"\"\n",
    "    Evaluate the average endpoint error w.r.t the ground truth flow_gt.\n",
    "    Excludes pixels, where u or v components of flow_gt have values > 1e9.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert (flow.dim() == 4 and flow_gt.dim() == 4)\n",
    "    assert (flow.size(1) == 2 and flow_gt.size(1) == 2)\n",
    "\n",
    "    u = flow[0,0,:,:] * flow[0,0,:,:].le(1e9).float()\n",
    "    v = flow[0,1,:,:] * flow[0,1,:,:].le(1e9).float()\n",
    "    #u = flow[0,0,:,:]\n",
    "    #v = flow[0,1,:,:]\n",
    "    \n",
    "    # Filter for invalid pixels\n",
    "    u_gt = flow_gt[0,0,:,:] * flow_gt[0,0,:,:].le(1e9).float()\n",
    "    v_gt = flow_gt[0,1,:,:] * flow_gt[0,1,:,:].le(1e9).float()\n",
    "\n",
    "    # AEPE for each pixel\n",
    "    errors = np.sqrt((u-u_gt)**2 + (v-v_gt)**2)\n",
    "    \n",
    "    #print(errors)\n",
    "    \n",
    "    # Average over all pixels\n",
    "    aepe = sum(sum(errors))/errors.nelement()\n",
    "\n",
    "    return aepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image(im, flow):\n",
    "    \"\"\" Warps given image according to the given optical flow.\"\"\"\n",
    "    assert (im.dim() == 4 and flow.dim() == 4)\n",
    "    assert (im.size(1) in [1,3] and flow.size(1) == 2)\n",
    "    \n",
    "    # Permute dimensions and range of flow for grid_sample()\n",
    "    norm_flow = torch.Tensor(flow.permute(0,2,3,1))\n",
    "    # You have to add the meshgrid according to this:\n",
    "    # https://discuss.pytorch.org/t/warp-video-frame-from-optical-flow/6013/4\n",
    "    W=norm_flow.size(1)\n",
    "    H=norm_flow.size(2)    \n",
    "    \n",
    "    meshH, meshW= torch.meshgrid([torch.linspace(0,W-1,W), torch.linspace(0,H-1,H)])\n",
    "    meshH=torch.t(meshH).reshape(1,1,H,W)\n",
    "    meshW=torch.t(meshW).reshape(1,1,H,W)\n",
    "    grid = torch.cat((meshW,meshH),1).permute(0,3,2,1)\n",
    "       \n",
    "    norm_flow = norm_flow + grid\n",
    "    \n",
    "    norm_flow[:,:,:,0] = norm_flow[:,:,:,0] / (norm_flow.size(2)-1) # normalize it to (0,1)\n",
    "    norm_flow[:,:,:,1] = norm_flow[:,:,:,1] / (norm_flow.size(1)-1) # normalize it to (0,1)\n",
    "    \n",
    "    # Clip it to range (-1,1) for the grid_sample function\n",
    "    norm_flow = norm_flow*2 - 1 \n",
    "    \n",
    "    # Interpolate the image with the flow\n",
    "    warped = tf.grid_sample(im, norm_flow)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_warping_practice(im1, im2, flow_gt):\n",
    "    \"\"\" Visualizes the result of warping the second image by ground truth.\"\"\"\n",
    "    print(\"\\nWarping practice ...\")\n",
    "    assert (im1.dim() == 4 and im2.dim() == 4 and flow_gt.dim() == 4)\n",
    "    assert (im1.size(1) in [1,3] and im2.size(1) in [1,3] and flow_gt.size(1) == 2)\n",
    "    \n",
    "    warped_im2 = warp_image(im2, flow_gt)\n",
    "    \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Image 1\")\n",
    "    plt.imshow(im1[0,0,:,:], cmap=\"gray\")\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Warped Image 2\")\n",
    "    plt.imshow(warped_im2[0,0,:,:], cmap=\"gray\")\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Difference\")\n",
    "    plt.imshow(im1[0,0,:,:]-warped_im2[0,0,:,:], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_hs(im1, im2, flow, lambda_hs):\n",
    "    \"\"\" Evalutes Horn-Schunck energy function.\"\"\"\n",
    "    assert (im1.dim() == 4 and im2.dim() == 4 and flow.dim() == 4)\n",
    "    assert (im1.size(1) == 1 and im2.size(1) == 1 and flow.size(1) == 2)\n",
    "\n",
    "    # (Using sum instead of integral (discrete pixels in picture))\n",
    "    It1 = warp_image(im2,flow)\n",
    "    It0 = im1\n",
    "    \n",
    "    # Apply Sobel filter on u(x,y) and v(x,y) to get their gradients    \n",
    "    \n",
    "    #flipped sobels\n",
    "    sobel_filter_x = torch.Tensor([[[[1., 0. , -1.], [2., 0., -2.], [1., 0. , -1.]]]])/4\n",
    "    sobel_filter_y = torch.Tensor([[[ [1., 2., 1.], [0., 0., 0.],[-1., -2., -1.]]]])/4\n",
    "    \n",
    "    \n",
    "    u = flow[:,0,:,:].unsqueeze(0)\n",
    "    v = flow[:,1,:,:].unsqueeze(0)\n",
    "\n",
    "    \n",
    "    du_dx = tf.conv2d(u, sobel_filter_x, padding=1)    \n",
    "    du_dy = tf.conv2d(u, sobel_filter_y, padding=1)                                         \n",
    "    dv_dx = tf.conv2d(v, sobel_filter_x, padding=1)\n",
    "    dv_dy = tf.conv2d(v, sobel_filter_y, padding=1)\n",
    "    \n",
    "    \n",
    "                                             \n",
    "    energy = torch.sum((It1-It0)**2) + torch.sum(lambda_hs*(du_dx**2+du_dy**2+(dv_dx**2+dv_dy**2)))\n",
    "\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_flow(im1, im2, flow_gt, lambda_hs, learning_rate, num_iter):\n",
    "    \"\"\"\n",
    "    Estimate flow using HS with Gradient Descent.\n",
    "    Displays average endpoint error.\n",
    "    Visualizes flow field.\n",
    "\n",
    "    Returns estimated flow]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nGradient Descent ...\")\n",
    "    \n",
    "    assert (im1.dim() == 4 and im2.dim() == 4 and flow_gt.dim() == 4)\n",
    "    assert (im1.size(1) == 1 and im2.size(1) == 1 and flow_gt.size(1) == 2)\n",
    "\n",
    "    # Initial flow\n",
    "    flow = torch.rand(1,2,flow_gt.size(2),flow_gt.size(3),requires_grad=True)\n",
    "    \n",
    "    for i in np.arange(0, num_iter):\n",
    "        \n",
    "        # Forward pass   \n",
    "        loss = energy_hs(im1,im2,flow,lambda_hs)\n",
    "        \n",
    "        if not i%50: print(\"Iteration: {} Loss: {}\".format(i+1, loss))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent step\n",
    "        update = flow.grad\n",
    "        with torch.no_grad():\n",
    "            flow -= update * learning_rate \n",
    "        flow.grad.zero_()\n",
    "    \n",
    "    print(\"Gradient Descent AEPE: \", evaluate_flow(flow.data, flow_gt))\n",
    "    with torch.no_grad():\n",
    "        plt.figure()\n",
    "        plt.title(\"Flow estimated with gradient descent\")\n",
    "        plt.imshow(flow2rgb(torch2numpy(flow.data.resize(flow.size(1),flow.size(2),flow.size(3)))))\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_flow_LBFGS(im1, im2, flow_gt, lambda_hs, learning_rate, num_iter):\n",
    "    \"\"\"\n",
    "    Estimates flow using HS with LBFGS.\n",
    "    Displays average endpoint error.\n",
    "    Visualizes flow field.\n",
    "\n",
    "    Returns estimated flow\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nLBFGS ...\")\n",
    "    \n",
    "    # Initial flow\n",
    "    flo = torch.rand(1,2,im1.size(2),im1.size(3))\n",
    "    flo.requires_grad = True\n",
    "    \n",
    "    global c\n",
    "    c = 0\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(params=[flo], lr=learning_rate, max_iter=num_iter)\n",
    "\n",
    "    \n",
    "    def closure():\n",
    "        global c\n",
    "        c =  c + 1\n",
    "        optimizer.zero_grad()\n",
    "        #loss = energy_hs(im1,im2,flo,lambda_hs).clone().detach().requires_grad_(True)\n",
    "        loss = energy_hs(im1,im2,flo,lambda_hs)\n",
    "        if not c%50: print(\"Iteration: {} Loss: {}\".format(c, loss))\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    assert (im1.dim() == 4 and im2.dim() == 4 and flow_gt.dim() == 4)\n",
    "    assert (im1.size(1) == 1 and im2.size(1) == 1 and flow_gt.size(1) == 2)\n",
    "    \n",
    "    print(\"LBFGS AEPE: \", evaluate_flow(flo.data, flow_gt))\n",
    "    with torch.no_grad():\n",
    "        plt.figure()\n",
    "        plt.title(\"Flow estimated with LBFGS\")\n",
    "        plt.imshow(flow2rgb(torch2numpy(flo.data.resize(flo.size(1),flo.size(2),flo.size(3)))))\n",
    "        plt.show()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_flow_coarse_to_fine(im1, im2, flow_gt, lambda_hs, learning_rate,\n",
    "                                 num_iter, num_level):\n",
    "    \"\"\"\n",
    "    Estimates flow using HS with LBFGS in a coarse-to-fine scheme.\n",
    "    Displays average endpoint error.\n",
    "    Visualizes flow field.\n",
    "\n",
    "    Returns estimated flow\n",
    "    \"\"\"\n",
    "    print(\"\\nCoarse to fine ...\")\n",
    "    \n",
    "    # Figure with subplots for plots\n",
    "    figg, axes = plt.subplots(1, num_level, figsize=(15,15))\n",
    "    \n",
    "    # Initial flow\n",
    "    flo = torch.rand(1,2,int(im1.size(2)/num_level),int(im1.size(3)/num_level))\n",
    "    flo.requires_grad = True\n",
    "    \n",
    "    # Loop over levels\n",
    "    for l in np.flip(np.arange(1, num_level+1)):\n",
    "        \n",
    "        og_size = (im1.shape[2],im1.shape[3])\n",
    "        down_size = (im1.shape[2]//l,im1.shape[3]//l)\n",
    "        \n",
    "        print(\"Level {}\".format(l))\n",
    "        sampling = torch.nn.Upsample(down_size, mode='bilinear', align_corners=False)\n",
    "        downsampled_im1 = sampling(im1)\n",
    "        downsampled_im2 = sampling(im2)\n",
    "        downsampled_flow_gt = sampling(flow_gt)\n",
    "        flo = sampling(flo).data.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        global c\n",
    "        c = 0\n",
    "        \n",
    "        optimizer = torch.optim.LBFGS(params=[flo], lr=learning_rate, max_iter=num_iter)\n",
    "        \n",
    "        def closure():\n",
    "            global c\n",
    "            c =  c + 1\n",
    "            optimizer.zero_grad()\n",
    "            loss = energy_hs(downsampled_im1, downsampled_im2,flo,lambda_hs)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        # Optimize flow for current level \n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Upsample current image (flow) from pyramid to full resolution\n",
    "            plot_scale_factor = num_level/l\n",
    "            sample = torch.nn.Upsample(og_size, mode='bilinear', align_corners=False)\n",
    "            full_res_flo = sample(flo)\n",
    "            print(\"LBFGS AEPE: \", evaluate_flow(full_res_flo.data, flow_gt))\n",
    "            # Plot\n",
    "            axes[l-1].imshow(flow2rgb(torch2numpy(full_res_flo.data.resize(full_res_flo.size(1),\n",
    "                                                                           full_res_flo.size(2),\n",
    "                                                                           full_res_flo.size(3)))))\n",
    "            axes[l-1].set_title(\"Level {}\".format(l))\n",
    "            \n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    assert (im1.dim() == 4 and im2.dim() == 4 and flow_gt.dim() == 4)\n",
    "    assert (im1.size(1) == 1 and im2.size(1) == 1 and flow_gt.size(1) == 2)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2():\n",
    "    \n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Loading data\n",
    "    im1, im2, flow_gt = load_data(\"frame10.png\", \"frame11.png\", \"flow10.flo\")\n",
    "\n",
    "\n",
    "    # Parameters\n",
    "    lambda_hs = 0.0015\n",
    "    num_iter = 400\n",
    "\n",
    "    # Warping_practice\n",
    "    visualize_warping_practice(im1, im2, flow_gt)\n",
    "        \n",
    "    # Gradient descent\n",
    "    learning_rate = 20.0\n",
    "    estimate_flow(im1, im2, flow_gt, lambda_hs, learning_rate, num_iter)\n",
    "\n",
    "    # LBFGS\n",
    "    learning_rate = 1.0\n",
    "    estimate_flow_LBFGS(im1, im2, flow_gt, lambda_hs, learning_rate, num_iter)\n",
    "\n",
    "    # Coarse to fine\n",
    "    learning_rate = 1.0\n",
    "    num_level = 4\n",
    "    estimate_flow_coarse_to_fine(\n",
    "        im1, im2, flow_gt, lambda_hs, learning_rate, num_iter, num_level)\n",
    "    \n",
    "    \"\"\"\n",
    "    AEPE for 3 lvl: 0.4804\n",
    "    AEPE for 4 lvl: 0.4636\n",
    "    AEPE for 5 lvl: 0.4580\n",
    "    AEPE for 6 lvl: 0.4556\n",
    "    AEPE for 7 lvl: 0.4555\n",
    "    \n",
    "    The AEPE is slightly decreasing using more levels for the coarse-to-fine estimation.\n",
    "    The output flow does not seem to change much for 4 or more levels.\n",
    "    The optimal trade-off between performance and computational effort might be using 5 levels.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problem2()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
