{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "using Optim\n",
    "using ImageFiltering\n",
    "using Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_to_grayscale (generic function with 1 method)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert an Float32 rbg image to Float64 grayscale image\n",
    "function convert_to_grayscale(I::Array{Float32,3})\n",
    "    I=convert(Array{Float64,3}, I)\n",
    "    I_gray = 0.2989*I[:,:,1] + 0.5870*I[:,:,2] + 0.1140*I[:,:,3]\n",
    "    return I_gray::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data (generic function with 1 method)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tsukuba disparity dataset and convert it to grayscale\n",
    "function load_data()\n",
    "    i0_path = string(@__DIR__,\"/i0.png\")\n",
    "    i0 = imread(i0_path)\n",
    "    i0 = convert_to_grayscale(i0)\n",
    "    i1_path = string(@__DIR__,\"/i1.png\")\n",
    "    i1 = imread(i1_path)\n",
    "    i1 = convert_to_grayscale(i1)\n",
    "    gt_path = string(@__DIR__,\"/gt.png\")\n",
    "    gt64 = convert(Array{Float64,2}, imread(gt_path)*255)\n",
    "\n",
    "    @assert maximum(gt64) <= 16\n",
    "    return i0::Array{Float64,2}, i1::Array{Float64,2}, gt64::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_studentt (generic function with 1 method)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function log_studentt(x::Float64, alpha::Float64, sigma::Float64)\n",
    "    \n",
    "    #TODO: Change the code to accept matrix input\n",
    "    \n",
    "    function log_student(x, alpha, sigma)\n",
    "        return log((1.0 + (1.0/(2.0*sigma^2))*x^2)^(-1.0*alpha))\n",
    "    end\n",
    "    \n",
    "    function log_grad_student(x, alpha, sigma)\n",
    "        return -(2.0*alpha*x)/((2.0*sigma^2)+x^2)\n",
    "    end\n",
    "    \n",
    "    value = log_student(x, alpha, sigma)\n",
    "    grad = log_grad_student(x, alpha, sigma)\n",
    "\n",
    "    return value::Float64, grad::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stereo_log_prior (generic function with 1 method)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate stereo log prior.\n",
    "# Set: alpha=1.0, sigma=1.0\n",
    "function stereo_log_prior(x::Array{Float64,2})\n",
    "    alpha = 1.0\n",
    "    sigma = 1.0\n",
    "    \n",
    "    value = 0\n",
    "    grad = zeros(size(x))\n",
    "    for i = 1:size(x)[1]-1#Height?\n",
    "        for j = 1:size(x)[2]-1#Width?\n",
    "            value += log_studentt(x[i+1, j] - x[i,j], alpha, sigma)[1]\n",
    "            value += log_studentt(x[i, j+1] - x[i,j], alpha, sigma)[1]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Partial derivative to every pixel\n",
    "    for k = 1:size(x)[1]\n",
    "        for l = 1:size(x)[2]\n",
    "            if k + 1 <= size(x)[1]\n",
    "                grad[k,l] += log_studentt(x[k,l]-x[k+1,l], alpha, sigma)[2] \n",
    "            end\n",
    "            if k - 1 >= 1\n",
    "                grad[k,l] += log_studentt(x[k,l]-x[k-1,l], alpha, sigma)[2]\n",
    "            end\n",
    "            if l + 1 <= size(x)[2]\n",
    "                grad[k,l] += log_studentt(x[k,l]-x[k,l+1], alpha, sigma)[2]\n",
    "            end\n",
    "            if l - 1 >= 1\n",
    "                grad[k,l] += log_studentt(x[k,l]-x[k,l-1], alpha, sigma)[2]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return  value::Float64, grad::Array{Float64,2}\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stereo_log_likelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate stereo log likelihood.\n",
    "# Set: Alpha = 1.0, Sigma = 0.004\n",
    "function stereo_log_likelihood(x::Array{Float64,2}, im0::Array{Float64,2}, im1::Array{Float64,2})\n",
    "    alpha = 1.0\n",
    "    sigma = 0.004\n",
    "    value = 0\n",
    "    grad = zeros(size(im1))\n",
    "    \n",
    "    # We need the horizontal image derivative from I1 to calculate the gradient of the LH\n",
    "    kernel = centered([1.0 0.0 -1.0; 2.0 0.0 -2.0; 1.0 0.0 -1.0]);\n",
    "    sobel_x = imfilter(im1, kernel);\n",
    "    h_img_grad = imfilter(sobel_x, kernel')    \n",
    "    \n",
    "    # We need to interpolate im1 and the horizontal image derivative,\n",
    "    # as we use continous optimization and might have continous disparities now\n",
    "    im1 = interpolate(im1, BSpline(Linear()))\n",
    "    h_img_grad = interpolate(h_img_grad,  BSpline(Linear()))\n",
    "\n",
    "\n",
    "    for i = 1:size(x)[1]\n",
    "        for j = 1:size(x)[2]\n",
    "            # Substract value of Im0 from value of (left) shifted Im1\n",
    "            d = im0[i,j]-im1[i-x[i, j], j]\n",
    "            value += log_studentt(d, alpha, sigma)[1]\n",
    "            grad[i,j] = (-1)*log_studentt(d, alpha, sigma)[2]*h_img_grad[i-x[i, j], j]          \n",
    "        end\n",
    "    end\n",
    "    return value::Float64, grad::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stereo_log_posterior (generic function with 1 method)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate stereo posterior\n",
    "function stereo_log_posterior(x::Array{Float64,2}, im0::Array{Float64,2}, im1::Array{Float64,2}) \n",
    "    #log(posterior) = log(prior*LH) = log(prior) + log(LH)\n",
    "    # (We can drop the marginalisation terms)\n",
    "    log_posterior = stereo_log_likelihood(x, im0, im1)[1] + stereo_log_prior(x)[1] #+ stereo_log_prior(im0)[1]\n",
    "    grad_lh = stereo_log_likelihood(x, im0, im1)[2]\n",
    "    grad_x = stereo_log_prior(x)[2]\n",
    "    # (Derivative of I0 to x should be zero, so we drop it ...)\n",
    "    log_posterior_grad = grad_lh + grad_x\n",
    "    \n",
    "    # Instead of maximizing the log (to 0), we want to minimize the - log (to 0)\n",
    "    return -log_posterior::Float64, -log_posterior_grad::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stereo (generic function with 1 method)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run stereo algorithm using gradient ascent or sth similar\n",
    "function stereo(x0::Array{Float64,2}, im0::Array{Float64,2}, im1::Array{Float64,2})\n",
    "    # Helper function with fixed im0 and im1\n",
    "    function f(y)\n",
    "        value = stereo_log_posterior(y, im0, im1)[1]\n",
    "        return value\n",
    "    end\n",
    "    # Helper function for gradients\n",
    "    function g!(storage, y)\n",
    "        grad = stereo_log_posterior(y, im0, im1)[2]\n",
    "        storage[:,:] = grad\n",
    "    end\n",
    "    # Minimum 0 disparity\n",
    "    lower = zeros(size(x0))\n",
    "    # Maximum 14.0 disparity\n",
    "    upper = fill(14.0, size(x0))\n",
    "    # Once differentiable\n",
    "    od = OnceDifferentiable(f, g!, x0)\n",
    "    options = Optim.Options(iterations=10, show_trace=true, allow_f_increases=true)\n",
    "    # Specify optim algorithm here\n",
    "    #res = optimize(f, g!, x0, ConjugateGradient(), options)\n",
    "    #res = optimize(od, x0, lower, upper, GradientDescent(), options)\n",
    "    results = optimize(od, lower, upper, x0, Fminbox(GradientDescent()); iterations = 20)\n",
    "    x = Optim.minimizer(res)\n",
    "    return x::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "constant_disparity (generic function with 1 method)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create constant disparity of all 8's of size DISPARITY_SIZE\n",
    "function constant_disparity(disparity_size::Tuple{Int64,Int64})\n",
    "    disparity_map = fill(8.0, disparity_size)\n",
    "    return disparity_map::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_disparity (generic function with 1 method)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random disparity in [0,14] of size DISPARITY_SIZE\n",
    "# We changed DISPARITY_SIZE to a tuple of integers\n",
    "function random_disparity(disparity_size::Tuple{Int64,Int64})\n",
    "    disparity_map = Array{Float64,2}(rand(collect(1:14),disparity_size))\n",
    "    return disparity_map::Array{Float64,2}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem3 (generic function with 1 method)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function problem3()\n",
    "    # use problem 2's load_data\n",
    "    im0, im1, gt = load_data()\n",
    "    \n",
    "    # Display stereo: Initialized with constant 8's\n",
    "    const_d = constant_disparity(size(gt))\n",
    "    \n",
    "    # Display stereo: Initialized with noise in [0,14]\n",
    "    random_d = random_disparity(size(gt))\n",
    "    \n",
    "    # print(\"Prior GT disparity: \", stereo_log_prior(gt)[1])\n",
    "    # println(\"\\n Prior constant disparity: \", stereo_log_prior(const_d)[1])\n",
    "    # println(\"\\n Prior random disparity: \", stereo_log_prior(random_d)[1])\n",
    "    # print(\"\\nLH const disp: \", stereo_log_likelihood(const_d, im0, im1)[1])\n",
    "    #x = stereo(random_d, im0, im1)\n",
    "    ## println(\"We got here\")\n",
    "    #figure()\n",
    "    #title(\"Estimated disparity map\")\n",
    "    #imshow(x, \"gray\")\n",
    "    #show()\n",
    "    #figure()\n",
    "    #title(\"Ground truth disparity map\")\n",
    "    #imshow(gt, \"gray\")\n",
    "    #show()\n",
    "    #imshow(shift_disparity(im0, gt), \"gray\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # print(\"\\nLH random disp: \", stereo_log_likelihood(random_d, im0, im1)[1])\n",
    "    \n",
    "    # Display stereo: Initialized with gt\n",
    "    # print(\"\\nLH GT disp: \", stereo_log_likelihood(gt, im0, im1)[1])\n",
    "    \n",
    "    # Coarse to fine estimation..\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching optimize(::OnceDifferentiable{Float64,Array{Float64,2},Array{Float64,2}}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Fminbox{GradientDescent{LineSearches.InitialPrevious{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Nothing,getfield(Optim, Symbol(\"##12#14\"))},Float64,getfield(Optim, Symbol(\"##46#48\"))}, ::Optim.Options{Float64,Nothing}; iterations=20)\nClosest candidates are:\n  optimize(::OnceDifferentiable, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox, ::Any) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:175 got unsupported keyword argument \"iterations\"\n  optimize(::Any, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox, ::Any; inplace, autodiff) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:163 got unsupported keyword argument \"iterations\"\n  optimize(::OnceDifferentiable, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:175 got unsupported keyword argument \"iterations\"\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching optimize(::OnceDifferentiable{Float64,Array{Float64,2},Array{Float64,2}}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Fminbox{GradientDescent{LineSearches.InitialPrevious{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Nothing,getfield(Optim, Symbol(\"##12#14\"))},Float64,getfield(Optim, Symbol(\"##46#48\"))}, ::Optim.Options{Float64,Nothing}; iterations=20)\nClosest candidates are:\n  optimize(::OnceDifferentiable, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox, ::Any) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:175 got unsupported keyword argument \"iterations\"\n  optimize(::Any, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox, ::Any; inplace, autodiff) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:163 got unsupported keyword argument \"iterations\"\n  optimize(::OnceDifferentiable, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::AbstractArray{T<:AbstractFloat,N} where N, ::Fminbox) where T<:AbstractFloat at /home/yannik/.julia/packages/Optim/Agd3B/src/multivariate/solvers/constrained/fminbox.jl:175 got unsupported keyword argument \"iterations\"\n  ...",
      "",
      "Stacktrace:",
      " [1] kwerr(::NamedTuple{(:iterations,),Tuple{Int64}}, ::Function, ::OnceDifferentiable{Float64,Array{Float64,2},Array{Float64,2}}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Fminbox{GradientDescent{LineSearches.InitialPrevious{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Nothing,getfield(Optim, Symbol(\"##12#14\"))},Float64,getfield(Optim, Symbol(\"##46#48\"))}, ::Optim.Options{Float64,Nothing}) at ./error.jl:125",
      " [2] (::getfield(Optim, Symbol(\"#kw##optimize\")))(::NamedTuple{(:iterations,),Tuple{Int64}}, ::typeof(optimize), ::OnceDifferentiable{Float64,Array{Float64,2},Array{Float64,2}}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Fminbox{GradientDescent{LineSearches.InitialPrevious{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Nothing,getfield(Optim, Symbol(\"##12#14\"))},Float64,getfield(Optim, Symbol(\"##46#48\"))}, ::Optim.Options{Float64,Nothing}) at ./none:0 (repeats 2 times)",
      " [3] stereo(::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}) at ./In[157]:23",
      " [4] problem3() at ./In[160]:15",
      " [5] top-level scope at In[161]:1"
     ]
    }
   ],
   "source": [
    "problem3()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
